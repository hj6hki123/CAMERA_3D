{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d96dd5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\klooom\\miniconda3\\envs\\torchpre\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# In[1]: 環境設定與套件載入\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # 關閉 Huggingface parallel 警告\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import ndcg_score, average_precision_score\n",
    "\n",
    "sns.set(style=\"whitegrid\", font=\"Arial\")\n",
    "\n",
    "# 根據專案結構調整 import\n",
    "from datasets.unified_dataset import UnifiedDataset\n",
    "from models.rag_text_encoder import RAGTextEncoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dbf76ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[2]: 參數設定（可依需要調整）\n",
    "data_jsonl = \"datasets/selected_data.jsonl\"\n",
    "cache_dir  = \"ckpts_0622\"         # Stage1 模型與快取目錄\n",
    "val_size   = 1000            # 驗證集大小\n",
    "batch_size = 16\n",
    "topk       = 4\n",
    "device     = \"cuda\"          # 或 \"cpu\"\n",
    "\n",
    "# 建立目錄\n",
    "os.makedirs(cache_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b038743e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertAdapterModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['heads.default.3.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "There are adapters available but none are activated for the forward pass.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cached memory_vec (ckpts_0622\\temp_corpus.jsonl.pt)\n",
      "模型與檢索器載入完成。\n"
     ]
    }
   ],
   "source": [
    "# In[3]: 載入文字編碼器模型\n",
    "dev = torch.device(device)\n",
    "ckpt = torch.load(f\"{cache_dir}/enc1_ep19.pth\", map_location=dev)\n",
    "\n",
    "txt_enc = RAGTextEncoder(\n",
    "    unified_jsonl=data_jsonl,\n",
    "    top_k=topk,\n",
    "    device=device,\n",
    "    cache_dir=cache_dir\n",
    ").to(dev)\n",
    "txt_enc.load_state_dict(ckpt[\"txt\"])\n",
    "txt_enc.eval()\n",
    "\n",
    "retriever = txt_enc.retriever\n",
    "fusion    = txt_enc.fusion\n",
    "\n",
    "print(\"模型與檢索器載入完成。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b77c1bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7370\n",
      "驗證集樣本數：1000\n"
     ]
    }
   ],
   "source": [
    "# In[4]: 準備驗證集 DataLoader\n",
    "full_ds = UnifiedDataset(data_jsonl, num_views=12)\n",
    "print(len(full_ds))\n",
    "val_ds  = Subset(full_ds, list(range(val_size)))\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n",
    "print(f\"驗證集樣本數：{len(val_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b360f9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Corpus size: 215498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Baseline 指標: 100%|██████████| 63/63 [01:01<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Recall@K': 0.124, 'NDCG@K': 0.054183911231882126, 'mAP': 0.014207950581950415, 'MRR@K': 0.08891666666666666}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # In[5]: 批次化基線檢索指標計算\n",
    "# @torch.no_grad()\n",
    "# def rebuild_memory(retriever, batch_size=64, device=\"cuda\"):\n",
    "#     \"\"\"\n",
    "#     以當前 retriever.kenc 權重重新編碼 corpus caption，\n",
    "#     並覆寫 retriever.memory_vec in-place。\n",
    "#     \"\"\"\n",
    "#     retriever.kenc.eval().to(device)\n",
    "\n",
    "#     new_vecs = []\n",
    "#     for i in range(0, len(retriever.texts), batch_size):\n",
    "#         chunk = retriever.texts[i:i+batch_size]\n",
    "#         enc   = retriever.tok(chunk,\n",
    "#                               return_tensors=\"pt\",\n",
    "#                               padding=True,\n",
    "#                               truncation=True).to(device)\n",
    "#         out   = retriever.kenc(**enc).last_hidden_state[:, 0]  # CLS\n",
    "#         new_vecs.append(F.normalize(out, 2, -1).cpu())\n",
    "\n",
    "#     new_memory = torch.cat(new_vecs, 0)             # (N,768)\n",
    "#     retriever.memory_vec.data.copy_(new_memory)     # 就地覆寫\n",
    "# rebuild_memory(txt_enc.retriever, batch_size=128, device=device)\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_baseline_batch(retriever, dataset, topk, batch_size, device):\n",
    "    retriever.qenc.eval()\n",
    "    retriever.kenc.eval()\n",
    "    retriever.eval()\n",
    "\n",
    "    mem_vec = F.normalize(retriever.memory_vec, 2, -1).to(device)  # (N,768)\n",
    "    print(\">> Corpus size:\", len(retriever.obj_ids))\n",
    "\n",
    "    recalls, ndcgs, aps, rrs = [], [], [], []\n",
    "    loader = DataLoader(dataset, batch_size=batch_size,\n",
    "                        shuffle=False, num_workers=4)\n",
    "\n",
    "    for caps, _, obj_ids, _ in tqdm(loader, desc=\"Baseline 指標\"):\n",
    "        # 一併正規化 query\n",
    "        q_vec = F.normalize(retriever.query_encode(list(caps)), 2, -1).to(device)  # (b,768)\n",
    "        sims  = q_vec @ mem_vec.T                                                   # (b,N)\n",
    "        sims_np = sims.cpu().numpy()\n",
    "        idx_topk = sims.topk(topk, dim=-1).indices.cpu().numpy()                   # (b,topk)\n",
    "\n",
    "        all_ids = np.array(retriever.obj_ids)\n",
    "        for i, oid in enumerate(obj_ids):\n",
    "            rel = (all_ids == oid).astype(int)\n",
    "            rank_idx = idx_topk[i]\n",
    "\n",
    "            # 計算指標...\n",
    "            recalls.append(int(rel[rank_idx].sum()>0))\n",
    "            ndcgs.append(ndcg_score([rel],[sims_np[i]], k=topk))\n",
    "            aps.append(average_precision_score(rel, sims_np[i]))\n",
    "            # MRR\n",
    "            rr=0.\n",
    "            for pos, gid in enumerate(rank_idx, start=1):\n",
    "                if rel[gid]:\n",
    "                    rr=1./pos; break\n",
    "            rrs.append(rr)\n",
    "\n",
    "    return {\n",
    "        \"Recall@K\": np.mean(recalls),\n",
    "        \"NDCG@K\":   np.mean(ndcgs),\n",
    "        \"mAP\":      np.mean(aps),\n",
    "        \"MRR@K\":    np.mean(rrs)\n",
    "    }\n",
    "# rebuild_memory(txt_enc.retriever, batch_size=128, device=device)\n",
    "metrics = compute_baseline_batch(\n",
    "    retriever=txt_enc.retriever,\n",
    "    dataset=val_ds,\n",
    "    topk=topk,\n",
    "    batch_size=batch_size,\n",
    "    device=device\n",
    ")\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33f47b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "蒐集注意力:   0%|          | 0/63 [00:11<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MaskedLMOutput' object has no attribute 'last_hidden_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(records)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# In[6]: 蒐集注意力矩陣\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m df_attn \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfusion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopk\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m共蒐集\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df_attn), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m筆 attention 紀錄\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     44\u001b[0m df_attn\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m, in \u001b[0;36mcollect_attention\u001b[1;34m(retriever, fusion, loader, device, topk)\u001b[0m\n\u001b[0;32m     13\u001b[0m     flat\u001b[38;5;241m.\u001b[39mextend(ctx[i])\n\u001b[0;32m     14\u001b[0m enc \u001b[38;5;241m=\u001b[39m retriever\u001b[38;5;241m.\u001b[39mtok(flat, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m                     padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 16\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqenc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43menc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_hidden_state\u001b[49m[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     17\u001b[0m tok_seq \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(B, topk\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 3) CrossFusion attention (avg over heads)\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MaskedLMOutput' object has no attribute 'last_hidden_state'"
     ]
    }
   ],
   "source": [
    "def collect_attention(retriever, fusion, loader, device, topk):\n",
    "    records = []\n",
    "    for caps, _, obj_ids, samp_idxs in tqdm(loader, desc=\"蒐集注意力\"):\n",
    "        B = len(caps)\n",
    "        # 1) 檢索 topk context\n",
    "        q_vec, sims, idx_topk, ctx, _ = retriever(\n",
    "            list(caps), list(obj_ids), topk=topk\n",
    "        )\n",
    "        # 2) 組 tok_seq\n",
    "        flat = []\n",
    "        for i in range(B):\n",
    "            flat.append(caps[i])\n",
    "            flat.extend(ctx[i])\n",
    "        enc = retriever.tok(flat, return_tensors=\"pt\",\n",
    "                            padding=True, truncation=True).to(device)\n",
    "        out = retriever.qenc(**enc).last_hidden_state[:, 0]\n",
    "        tok_seq = out.view(B, topk+1, -1)\n",
    "\n",
    "        # 3) CrossFusion attention (avg over heads)\n",
    "        with torch.no_grad():\n",
    "            _, all_attn = fusion(tok_seq)\n",
    "        # all_attn[l] shape = (B, T, T)\n",
    "\n",
    "        for b in range(B):\n",
    "            for l, attn_mat in enumerate(all_attn):\n",
    "                arr = attn_mat[b].cpu().numpy()  # shape = (T, T)\n",
    "                for c in range(1, topk+1):\n",
    "                    records.append({\n",
    "                        \"sample\":  samp_idxs[b].item(),\n",
    "                        \"layer\":   l,\n",
    "                        \"ctx_pos\": c-1,\n",
    "                        \"attn\":    arr[0, c],    # CLS idx = 0\n",
    "                        \"is_pos\":  int(\n",
    "                            retriever.obj_ids[idx_topk[b, c-1]] == obj_ids[b]\n",
    "                        )\n",
    "                    })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# In[6]: 蒐集注意力矩陣\n",
    "df_attn = collect_attention(\n",
    "    retriever, fusion, val_loader, dev, topk\n",
    ")\n",
    "print(\"共蒐集\", len(df_attn), \"筆 attention 紀錄\")\n",
    "df_attn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7178c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[7]: 繪製注意力分布盒鬚圖\n",
    "def plot_attention_box_avg(df, out_dir):\n",
    "    # Map is_pos to human-readable labels\n",
    "    df[\"label\"] = df[\"is_pos\"].map({1: \"Positive Sample\", 0: \"Negative Sample\"})\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(\n",
    "        data=df,\n",
    "        x=\"layer\",\n",
    "        y=\"attn\",\n",
    "        hue=\"label\"\n",
    "    )\n",
    "    plt.title(\"CrossFusion CLS→Contexts Attention Distribution (Average Heads)\")\n",
    "    plt.legend(title=\"Sample Type\", loc=\"upper right\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(out_dir, \"attention_boxplot_avg.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# Execute the plot function\n",
    "plot_attention_box_avg(df_attn, cache_dir)\n",
    "print(f\"Attention boxplot saved to {cache_dir}\")\n",
    "display(df_attn.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef6fa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hit_rate_by_layer(df):\n",
    "    \"\"\"\n",
    "    計算每一層中，CLS→context 的最強注意力是否命中正樣本，\n",
    "    並對每層匯出整體命中率。\n",
    "    \"\"\"\n",
    "    recs = []\n",
    "    # 按 sample + layer 分組\n",
    "    for (s, l), g in df.groupby([\"sample\", \"layer\"]):\n",
    "        # 找出該組裡 attn 最大的那一筆索引\n",
    "        idx_max = g[\"attn\"].idxmax()\n",
    "        hit     = int(g.loc[idx_max, \"is_pos\"])\n",
    "        recs.append({\"layer\": l, \"hit\": hit})\n",
    "    df2 = pd.DataFrame(recs)\n",
    "    # 算每層的平均命中率\n",
    "    return df2.groupby(\"layer\")[\"hit\"].mean().reset_index(name=\"hit_rate\")\n",
    "\n",
    "# 執行\n",
    "hit_layer = compute_hit_rate_by_layer(df_attn)\n",
    "hit_layer.to_csv(f\"{cache_dir}/hit_rate_by_layer.csv\", index=False)\n",
    "print(\"各層命中率：\\n\", hit_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9811503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[9]: 隨機案例注意力熱圖（修正版，平均後注意力）\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def plot_example_heatmaps(\n",
    "    retriever, fusion, dataset,\n",
    "    cache_dir, topk, device, num=5\n",
    "):\n",
    "    os.makedirs(f\"{cache_dir}/examples\", exist_ok=True)\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    cnt = 0\n",
    "\n",
    "    for caps, _, obj_ids, samp_idxs in loader:\n",
    "        q = caps[0]\n",
    "        with torch.no_grad():\n",
    "            # 1) 搜 topk context\n",
    "            _, _, idx_topk, ctx, _ = retriever(\n",
    "                [q], [obj_ids[0]], topk=topk\n",
    "            )\n",
    "            # 2) 組 flat list 丟進 BERT\n",
    "            flat = [q] + ctx[0]\n",
    "            enc = retriever.tok(\n",
    "                flat, return_tensors=\"pt\",\n",
    "                padding=True, truncation=True\n",
    "            ).to(device)\n",
    "            out = retriever.qenc(**enc).last_hidden_state[:,0]\n",
    "            tok_seq = out.view(1, topk+1, -1).to(device)\n",
    "\n",
    "            # 3) CrossFusion → all_attn（list of L, each shape=(1,T,T)）\n",
    "            _, all_attn = fusion(tok_seq)\n",
    "\n",
    "        # 4) 把所有層做平均\n",
    "        #    all_attn[l][0] shape=(T,T)\n",
    "        avg_attn = sum(attn[0].cpu().numpy() for attn in all_attn) / len(all_attn)\n",
    "        # CLS→contexts 注意力\n",
    "        cls2ctx = avg_attn[0, 1:]  # 跳過 CLS 位置\n",
    "\n",
    "        # 5) 畫 barplot\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.barplot(\n",
    "            x=list(range(topk)), y=cls2ctx,\n",
    "            palette=\"Blues_d\"\n",
    "        )\n",
    "        plt.xticks(\n",
    "            range(topk),\n",
    "            [t[:15]+\"…\" if len(t)>15 else t for t in ctx[0]],\n",
    "            rotation=30, ha=\"right\"\n",
    "        )\n",
    "        plt.ylabel(\"Average Attention to Contexts\")\n",
    "        plt.title(f\"Sample {samp_idxs.item()} CLS→Contexts\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{cache_dir}/examples/heat_{cnt}.png\")\n",
    "        plt.close()\n",
    "\n",
    "        cnt += 1\n",
    "        if cnt >= num:\n",
    "            break\n",
    "\n",
    "# 呼叫方式（確保把 topk, device, cache_dir 都帶進去）\n",
    "plot_example_heatmaps(\n",
    "    retriever=txt_enc.retriever,\n",
    "    fusion=txt_enc.fusion,\n",
    "    dataset=val_ds,\n",
    "    cache_dir=cache_dir,\n",
    "    topk=topk,\n",
    "    device=dev,\n",
    "    num=5\n",
    ")\n",
    "print(\"案例熱圖已存於\", f\"{cache_dir}/examples\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchpre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
